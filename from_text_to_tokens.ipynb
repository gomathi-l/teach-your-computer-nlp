{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "wntT4brdBWQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This notebook demonstrates how to implement tokenization using both Python’s built-in re module and spaCy.***"
      ],
      "metadata": {
        "id": "F-tWRD1kn60F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We start with the simplest approach of splitting on spaces, and then gradually expand the rules to include punctuation and other symbols. This helps illustrate how rule-based tokenization works, why empty tokens appear, and how small changes in the pattern can produce very different token sequences.\n",
        "- The goal is to show that regex-based tokenization is easy to implement and fully transparent, but also sensitive to the structure of the text."
      ],
      "metadata": {
        "id": "KyuE76X7BHIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://docs.python.org/3/library/re.html"
      ],
      "metadata": {
        "id": "coobVGaxpNBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"I'm lucky to pay only $5000 for my mother-in-law’s candies, chocolates in the U.S.!\"\n",
        "\n",
        "# Split on spaces\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(\"1:\", result)\n",
        "print(\"Total tokens 1:\", len(result))\n",
        "\n",
        "# Split on spaces and commas/periods\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(\"2:\", result)\n",
        "\n",
        "# Remove whitespace and drop empties\n",
        "result = [item for item in result if item.strip()]\n",
        "print(\"3:\", result)\n",
        "\n",
        "# Expanded punctuation\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(\"4:\", result)\n",
        "print(\"Total tokens (4):\", len(result))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvl2NAYl9ien",
        "outputId": "8fd66c13-b33e-4e00-e951-b49cb9d888a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: [\"I'm\", ' ', 'lucky', ' ', 'to', ' ', 'pay', ' ', 'only', ' ', '$5000', ' ', 'for', ' ', 'my', ' ', 'mother-in-law’s', ' ', 'candies,', ' ', 'chocolates', ' ', 'in', ' ', 'the', ' ', 'U.S.!']\n",
            "Total tokens 1: 27\n",
            "2: [\"I'm\", ' ', 'lucky', ' ', 'to', ' ', 'pay', ' ', 'only', ' ', '$5000', ' ', 'for', ' ', 'my', ' ', 'mother-in-law’s', ' ', 'candies', ',', '', ' ', 'chocolates', ' ', 'in', ' ', 'the', ' ', 'U', '.', 'S', '.', '!']\n",
            "3: [\"I'm\", 'lucky', 'to', 'pay', 'only', '$5000', 'for', 'my', 'mother-in-law’s', 'candies', ',', 'chocolates', 'in', 'the', 'U', '.', 'S', '.', '!']\n",
            "4: ['I', \"'\", 'm', 'lucky', 'to', 'pay', 'only', '$5000', 'for', 'my', 'mother-in-law’s', 'candies', ',', 'chocolates', 'in', 'the', 'U', '.', 'S', '.', '!']\n",
            "Total tokens (4): 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **spaCy**"
      ],
      "metadata": {
        "id": "1o7wz8vyBR0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "DtIBJSP7pTnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy is a Python library that helps in turning raw text into well-defined tokens. Instead of treating every word boundary the same, it reads text from left to right and applies a series of carefully designed rules to decide where tokens should begin and end.\n",
        "\n",
        "For example, abbreviations such as “U.S.” remain the same rather than being broken apart, while words like “we’re” are divided into “we” and “’re.” spaCy also pays attention to punctuation and special symbols: prefixes (e.g., “$5”), suffixes (e.g., “dogs’ ”), and infixes (e.g., “brother-in-law”).\n",
        "\n",
        "- Text is initially split using whitespace.\n",
        "- Prefixes are separated from the base word.\n",
        "- Exceptions and suffixes are identified and detached.\n",
        "- The result is a clean set of tokens that capture the structure of the text accurately."
      ],
      "metadata": {
        "id": "Es4brq6Svm-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of a longer paragraph.\n",
        "Scroll down further for a shorter example to understand it better."
      ],
      "metadata": {
        "id": "IjnAJKmIaZS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBfacA24uLXp",
        "outputId": "fb49f6fc-8f12-4313-e124-7c30940dcfa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "Formula\n",
            "One\n",
            "(\n",
            "F1\n",
            ")\n",
            "is\n",
            "the\n",
            "highest\n",
            "class\n",
            "of\n",
            "worldwide\n",
            "racing\n",
            "for\n",
            "\n",
            "\n",
            "open\n",
            "-\n",
            "wheel\n",
            "single\n",
            "-\n",
            "seater\n",
            "formula\n",
            "racing\n",
            "cars\n",
            "sanctioned\n",
            "by\n",
            "the\n",
            "Fédération\n",
            "\n",
            "\n",
            "Internationale\n",
            "de\n",
            "l'Automobile\n",
            "(\n",
            "FIA\n",
            ")\n",
            ".\n",
            "The\n",
            "FIA\n",
            "Formula\n",
            "One\n",
            "World\n",
            "Championship\n",
            "\n",
            "\n",
            "has\n",
            "been\n",
            "one\n",
            "of\n",
            "the\n",
            "world\n",
            "'s\n",
            "premier\n",
            "forms\n",
            "of\n",
            "motorsport\n",
            "since\n",
            "its\n",
            "inaugural\n",
            "\n",
            "\n",
            "running\n",
            "in\n",
            "1950\n",
            "and\n",
            "is\n",
            "often\n",
            "considered\n",
            "to\n",
            "be\n",
            "the\n",
            "pinnacle\n",
            "of\n",
            "motorsport\n",
            ".\n",
            "The\n",
            "\n",
            "\n",
            "word\n",
            "formula\n",
            "in\n",
            "the\n",
            "name\n",
            "refers\n",
            "to\n",
            "the\n",
            "set\n",
            "of\n",
            "rules\n",
            "all\n",
            "participant\n",
            "cars\n",
            "must\n",
            "\n",
            "\n",
            "follow\n",
            ".\n",
            "A\n",
            "Formula\n",
            "One\n",
            "season\n",
            "consists\n",
            "of\n",
            "a\n",
            "series\n",
            "of\n",
            "races\n",
            ",\n",
            "known\n",
            "as\n",
            "Grands\n",
            "\n",
            "\n",
            "Prix\n",
            ".\n",
            "Grands\n",
            "Prix\n",
            "take\n",
            "place\n",
            "in\n",
            "multiple\n",
            "countries\n",
            "and\n",
            "continents\n",
            "on\n",
            "either\n",
            "\n",
            "\n",
            "purpose\n",
            "-\n",
            "built\n",
            "circuits\n",
            "or\n",
            "closed\n",
            "roads\n",
            ".\n",
            "With\n",
            "the\n",
            "average\n",
            "annual\n",
            "cost\n",
            "of\n",
            "running\n",
            "\n",
            "\n",
            "a\n",
            "team\n",
            "—\n",
            "e.g.\n",
            ",\n",
            "designing\n",
            ",\n",
            "building\n",
            ",\n",
            "and\n",
            "maintaining\n",
            "cars\n",
            ";\n",
            "staff\n",
            "payroll\n",
            ";\n",
            "\n",
            "\n",
            "transport\n",
            "—\n",
            "at\n",
            "approximately\n",
            "£\n",
            "193\n",
            "million\n",
            "as\n",
            "of\n",
            "2018,[2\n",
            "]\n",
            "Formula\n",
            "One\n",
            "'s\n",
            "financial\n",
            "\n",
            "\n",
            "and\n",
            "political\n",
            "battles\n",
            "are\n",
            "widely\n",
            "reported\n",
            ".\n",
            "The\n",
            "Formula\n",
            "One\n",
            "Group\n",
            "is\n",
            "owned\n",
            "by\n",
            "\n",
            "\n",
            "Liberty\n",
            "Media\n",
            ",\n",
            "which\n",
            "acquired\n",
            "it\n",
            "in\n",
            "2017\n",
            "for\n",
            "US$\n",
            "8\n",
            "billion\n",
            ".\n",
            "The\n",
            "United\n",
            "Kingdom\n",
            "\n",
            "\n",
            "is\n",
            "the\n",
            "hub\n",
            "of\n",
            "Formula\n",
            "One\n",
            "racing\n",
            ",\n",
            "with\n",
            "six\n",
            "out\n",
            "of\n",
            "the\n",
            "ten\n",
            "teams\n",
            "based\n",
            "there\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text: https://en.wikipedia.org/wiki/Formula_One\n",
        "text = \"\"\"Formula One (F1) is the highest class of worldwide racing for\n",
        "open-wheel single-seater formula racing cars sanctioned by the Fédération\n",
        "Internationale de l'Automobile (FIA). The FIA Formula One World Championship\n",
        "has been one of the world's premier forms of motorsport since its inaugural\n",
        "running in 1950 and is often considered to be the pinnacle of motorsport. The\n",
        "word formula in the name refers to the set of rules all participant cars must\n",
        "follow. A Formula One season consists of a series of races, known as Grands\n",
        "Prix. Grands Prix take place in multiple countries and continents on either\n",
        "purpose-built circuits or closed roads. With the average annual cost of running\n",
        "a team—e.g., designing, building, and maintaining cars; staff payroll;\n",
        "transport—at approximately £193 million as of 2018,[2] Formula One's financial\n",
        "and political battles are widely reported. The Formula One Group is owned by\n",
        "Liberty Media, which acquired it in 2017 for US$8 billion. The United Kingdom\n",
        "is the hub of Formula One racing, with six out of the ten teams based there.\"\"\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"I'm lucky to pay only $5000 for my mother-in-law’s chocolates in the U.S.!\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W_TRyx2uS-H",
        "outputId": "93083ef1-98b8-4941-efc7-9f52301d10b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "I\n",
            "'m\n",
            "lucky\n",
            "to\n",
            "pay\n",
            "only\n",
            "$\n",
            "5000\n",
            "for\n",
            "my\n",
            "mother\n",
            "-\n",
            "in\n",
            "-\n",
            "law\n",
            "’s\n",
            "chocolates\n",
            "in\n",
            "the\n",
            "U.S.\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Byte Pair Encoding Tokenizer**"
      ],
      "metadata": {
        "id": "fdy9UT-NCHNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding (BPE) can be implemented from scratch to split words into subword units.\n",
        "It uses spaCy to tokenize text, then:\n",
        "- Builds a “vocabulary” of words broken down into individual characters plus an end-of-sequence marker.\n",
        "- Counts how often adjacent character pairs appear.\n",
        "- Repeatedly merges the most frequent pair, learning common letter clusters (subwords).\n",
        "- Applies those learned merges to new text so words are represented as a sequence of subword tokens."
      ],
      "metadata": {
        "id": "Q8waU9W4bJDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://huggingface.co/learn/llm-course/en/chapter6/5"
      ],
      "metadata": {
        "id": "lHmI2p6lAlHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the examples below, BPE is implemented from scratch so you can see exactly how it works: it builds a vocabulary from characters, counts frequent pairs, merges them step by step, and then applies those learned merges to new text. You can adjust the number of merges to see how the output changes. The first example uses a small Formula One–themed corpus, and the second uses a minimal toy corpus to show the process more clearly."
      ],
      "metadata": {
        "id": "SjZ7SVUwp97Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of a longer paragraph. Scroll down further for a shorter example to understand it better. I'd encourage you to play around with the number of merges to understand it better."
      ],
      "metadata": {
        "id": "9-ttVxMla1yX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1:**"
      ],
      "metadata": {
        "id": "0GwHkv0NqAyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [t.text.lower() for t in doc if not t.is_space]\n",
        "\n",
        "# Represent each word as a sequence of chars + </es> end marker\n",
        "def words_to_vocab(words):\n",
        "    vocab = Counter()\n",
        "    for w in words:\n",
        "        vocab[\" \".join(list(w) + [\"</es>\"])] += 1\n",
        "    return vocab\n",
        "\n",
        "# Count frequency of symbol pairs\n",
        "def get_freq(vocab):\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i+1])] += freq\n",
        "    return pairs\n",
        "\n",
        "# Merge the most frequent pair into a single symbol\n",
        "def merge_vocab(pair, vocab):\n",
        "    bigram = re.escape(\" \".join(pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    output = {}\n",
        "    for word, freq in vocab.items():\n",
        "        new_word = pattern.sub(\"\".join(pair), word)  # \"A B\" becomes \"AB\"\n",
        "        output[new_word] = output.get(new_word, 0) + freq\n",
        "    return output\n",
        "\n",
        "def learn_bpe(corpus_tokens, num_merges=50):\n",
        "    vocab = words_to_vocab(corpus_tokens)\n",
        "    merges = []\n",
        "    for _ in range(num_merges):\n",
        "        stats = get_freq(vocab)\n",
        "        if not stats:\n",
        "            break\n",
        "        best = max(stats, key=stats.get)\n",
        "        if stats[best] < 2:\n",
        "            break\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "        merges.append(best)\n",
        "    return merges\n",
        "\n",
        "def apply_bpe_word(word, merges_set):\n",
        "    symbols = list(word) + [\"</es>\"]\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        i = 0\n",
        "        while i < len(symbols) - 1:\n",
        "            pair = (symbols[i], symbols[i+1])\n",
        "            if pair in merges_set:\n",
        "                symbols[i:i+2] = [\"\".join(pair)]\n",
        "                changed = True\n",
        "            else:\n",
        "                i += 1\n",
        "    if symbols and symbols[-1] == \"</es>\":\n",
        "        symbols = symbols[:-1]\n",
        "    return symbols\n",
        "\n",
        "def apply_bpe(text, merges):\n",
        "    merges_set = set(merges)\n",
        "    toks = tokenize(text)\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if re.fullmatch(r\"[a-z0-9]+\", t):\n",
        "            out.extend(apply_bpe_word(t, merges_set))\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "corpus = \"\"\"\n",
        "Formula One teams race on street circuits like Monaco and purpose-built tracks like Silverstone.\n",
        "Ferrari and Mercedes have dominated different eras of Formula One, but Red Bull has been ascendant recently.\n",
        "Drivers compete for the World Championship across multiple Grands Prix in a season.\n",
        "\"\"\"\n",
        "\n",
        "# Train merges on corpus\n",
        "corpus_tokens = tokenize(corpus)\n",
        "merges = learn_bpe(corpus_tokens, num_merges=100)\n",
        "\n",
        "# Apply to a different sentence\n",
        "text = \"Formula One cars battle on purpose-built circuits and tight street tracks.\"\n",
        "bpe_tokens = apply_bpe(text, merges)\n",
        "\n",
        "print(\"Learnt merges (first 20):\", merges[:20])\n",
        "print(\"\\nOriginal:\", text)\n",
        "print(\"BPE tokens:\", bpe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWwL5SJIzajd",
        "outputId": "d968c3d9-dc18-4869-9202-5deb30e99a17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learnt merges (first 20): [('e', '</es>'), ('s', '</es>'), ('o', 'n'), ('e', 'r'), ('t', '</es>'), ('d', '</es>'), ('o', 'r'), ('u', 'l'), ('r', 'a'), ('e', 'n'), ('f', 'or'), ('m', 'ul'), ('a', '</es>'), ('on', 'e</es>'), ('r', 'e'), ('a', 'n'), ('.', '</es>'), ('r', 'i'), ('h', 'a'), ('for', 'mul')]\n",
            "\n",
            "Original: Formula One cars battle on purpose-built circuits and tight street tracks.\n",
            "BPE tokens: ['formula</es>', 'one</es>', 'c', 'a', 'r', 's</es>', 'b', 'a', 't', 't', 'l', 'e</es>', 'on</es>', 'p', 'u', 'r', 'p', 'os', 'e</es>', '-', 'b', 'ui', 'l', 't</es>', 'c', 'i', 'r', 'c', 'ui', 't', 's</es>', 'and</es>', 't', 'i', 'g', 'h', 't</es>', 'st', 're', 'e', 't</es>', 't', 'rac', 'k', 's</es>', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2:**"
      ],
      "metadata": {
        "id": "bHBU5QqvqDdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [t.text.lower() for t in doc if not t.is_space]\n",
        "\n",
        "# Represent each word as a sequence of chars + </es> end marker\n",
        "def words_to_vocab(words):\n",
        "    vocab = Counter()\n",
        "    for w in words:\n",
        "        vocab[\" \".join(list(w) + [\"</es>\"])] += 1\n",
        "    return vocab\n",
        "\n",
        "# Count frequency of symbol pairs\n",
        "def get_freq(vocab):\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i+1])] += freq\n",
        "    return pairs\n",
        "\n",
        "# Merge the most frequent pair into a single symbol\n",
        "def merge_vocab(pair, vocab):\n",
        "    bigram = re.escape(\" \".join(pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')  # whole-token pair\n",
        "    output = {}\n",
        "    for word, freq in vocab.items():\n",
        "        new_word = pattern.sub(\"\".join(pair), word)  # \"A B\" becomes \"AB\"\n",
        "        output[new_word] = output.get(new_word, 0) + freq\n",
        "    return output\n",
        "\n",
        "def learn_bpe(corpus_tokens, num_merges=2):\n",
        "    vocab = words_to_vocab(corpus_tokens)\n",
        "    merges = []\n",
        "    for _ in range(num_merges):\n",
        "        stats = get_freq(vocab)\n",
        "        if not stats:\n",
        "            break\n",
        "        best = max(stats, key=stats.get)\n",
        "        if stats[best] < 2:\n",
        "            break\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "        merges.append(best)\n",
        "    return merges\n",
        "\n",
        "def apply_bpe_word(word, merges_set):\n",
        "    symbols = list(word) + [\"</es>\"]\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        i = 0\n",
        "        while i < len(symbols) - 1:\n",
        "            pair = (symbols[i], symbols[i+1])\n",
        "            if pair in merges_set:\n",
        "                symbols[i:i+2] = [\"\".join(pair)]\n",
        "                changed = True\n",
        "            else:\n",
        "                i += 1\n",
        "    if symbols and symbols[-1] == \"</es>\":\n",
        "        symbols = symbols[:-1]\n",
        "    return symbols\n",
        "\n",
        "def apply_bpe(text, merges):\n",
        "    merges_set = set(merges)\n",
        "    toks = tokenize(text)\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if re.fullmatch(r\"[a-z0-9]+\", t):\n",
        "            out.extend(apply_bpe_word(t, merges_set))\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "corpus = \"\"\"\n",
        "car cars carpet bar bartender\n",
        "\"\"\"\n",
        "\n",
        "# Train merges on corpus\n",
        "corpus_tokens = tokenize(corpus)\n",
        "merges = learn_bpe(corpus_tokens, num_merges=3)\n",
        "\n",
        "# Apply to a different sentence\n",
        "text = \"I love cartier and barbie.\"\n",
        "bpe_tokens = apply_bpe(text, merges)\n",
        "\n",
        "print(\"Learnt merges (first n):\", merges[:3])\n",
        "print(\"\\nOriginal:\", text)\n",
        "print(\"BPE tokens:\", bpe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c4E4xG407EE",
        "outputId": "8c520f87-1c4f-4e19-ed8b-22e190de4237"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learnt merges (first n): [('a', 'r'), ('c', 'ar'), ('b', 'ar')]\n",
            "\n",
            "Original: I love cartier and barbie.\n",
            "BPE tokens: ['i', 'l', 'o', 'v', 'e', 'car', 't', 'i', 'e', 'r', 'a', 'n', 'd', 'bar', 'b', 'i', 'e', '.']\n"
          ]
        }
      ]
    }
  ]
}